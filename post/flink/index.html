<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Flink&amp;Table API &amp; SQL&amp;外部系统 | One-Piece</title>
<link rel="shortcut icon" href="https://onepiece-dz.github.io/favicon.ico?v=1596369584065">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://onepiece-dz.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Flink&amp;Table API &amp; SQL&amp;外部系统 | One-Piece - Atom Feed" href="https://onepiece-dz.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="前言
Table API &amp; SQL 有两种方式连接外部系统，一个是 table source，一个是 table sink，source 负责从外部系统获取数据，如：数据库，k-v 存储，消息队列或者文件系统；sink 负责将数据..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://onepiece-dz.github.io">
  <img class="avatar" src="https://onepiece-dz.github.io/images/avatar.png?v=1596369584065" alt="">
  </a>
  <h1 class="site-title">
    One-Piece
  </h1>
  <p class="site-description">
    知行合一
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
      
        <a href="/post/java" class="menu">
          java
        </a>
      
    
      
        <a href="/post" class="menu" target="_blank">
          flink
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              Flink&amp;Table API &amp; SQL&amp;外部系统
            </h2>
            <div class="post-info">
              <span>
                2020-05-31
              </span>
              <span>
                11 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="前言">前言</h1>
<p>Table API &amp; SQL 有两种方式连接外部系统，一个是 table source，一个是 table sink，source 负责从外部系统获取数据，如：数据库，k-v 存储，消息队列或者文件系统；sink 负责将数据提交到外部系统。依赖于source 和 sink 的类型，可能具有如下数据传输的格式：CSV, Parquet, or ORC。flink 为我们提供了连接上述外部系统的连接器，此外还支持我们自定义 source 和 sink，关于定义的内容可参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sourceSinks.html">用户自定义source&amp;sink</a></p>
<p>本章主要介绍如何创建一个 source 或 sink，并将其注册到 flink 中。</p>
<h1 id="依赖">依赖</h1>
<p>下表列出了所有可用的连接器和数据格式：</p>
<h2 id="连接器">连接器</h2>
<table>
<thead>
<tr>
<th>名称</th>
<th>版本</th>
<th>maven 依赖</th>
<th>SQL  客户端 jar</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filesystem</td>
<td></td>
<td>内部构建支持</td>
<td>内部构建支持</td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>6</td>
<td>flink-connector-elasticsearch6</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch6_2.11/1.10.0/flink-sql-connector-elasticsearch6_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.8</td>
<td>flink-connector-kafka-0.8</td>
<td>无</td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.9</td>
<td>flink-connector-kafka-0.9</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka-0.9_2.11/1.10.0/flink-sql-connector-kafka-0.9_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.10</td>
<td>flink-connector-kafka-0.10</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka-0.10_2.11/1.10.0/flink-sql-connector-kafka-0.10_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.11</td>
<td>flink-connector-kafka-0.11</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka-0.11_2.11/1.10.0/flink-sql-connector-kafka-0.11_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.11+ (universal)</td>
<td>flink-connector-kafka</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.11/1.10.0/flink-sql-connector-kafka_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>HBase</td>
<td>1.4.3</td>
<td>flink-hbase</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-hbase_2.11/1.10.0/flink-hbase_2.11-1.10.0.jar">下载</a></td>
</tr>
<tr>
<td>JDBC</td>
<td></td>
<td>flink-jdbc</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-jdbc_2.11/1.10.0/flink-jdbc_2.11-1.10.0.jar">下载</a></td>
</tr>
</tbody>
</table>
<p>##格式化方式</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>maven 依赖</th>
<th>SQL  客户端 jar</th>
</tr>
</thead>
<tbody>
<tr>
<td>Old CSV (for files)</td>
<td>内部构建支持</td>
<td>内部构建支持</td>
</tr>
<tr>
<td>CSV (for Kafka)</td>
<td>flink-csv</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-csv/1.10.0/flink-csv-1.10.0-sql-jar.jar">下载</a></td>
</tr>
<tr>
<td>JSON</td>
<td>flink-json</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-json/1.10.0/flink-json-1.10.0-sql-jar.jar">下载</a></td>
</tr>
<tr>
<td>Apache Avro</td>
<td>flink-avro</td>
<td><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-avro/1.10.0/flink-avro-1.10.0-sql-jar.jar">下载</a></td>
</tr>
</tbody>
</table>
<h1 id="概述">概述</h1>
<p>从 flink 1.6 之后，flink 将连接外部系统的实现与其申明进行了分离，类似于面向接口编程的编程思想，flink 基于此提供了不同的实现方式，我们可以根据自身情况进行选择，如：</p>
<ul>
<li>使用 Table API 编程语言进行定义，活着使用 DDL（SQL）语言进行定义；</li>
<li>在 SQL 客户端上使用 YAML 配置的方式。</li>
</ul>
<p>flink 的这种实现方式，不仅统一了 APIs 和 SQL 的使用，而且还有助于用户根据自身需求进行自定义实现，而不必去修改申明。  下面具体来讲一下申明的部分：</p>
<p>类似于传统关系型 sql 中的建表申明一样，flink 中的申明都包含以下几部分：表名、表的 schema、连接器，以及一个与外部系统的数据交换格式。<br>
其中<strong>连接器</strong>代表存储表数据的外部系统，如，kafka，文件系统的，有的外部系统定义的连接器可能已经包含了表的 schema。？？？</p>
<p>连接器可能支持多种不同的<strong>数据格式</strong>。例如，kafka 或者 文件系统可以使用 JSON、Avro、CSV等作为数据交换格式，而数据库连接器则可能直接使用表模式作为数据格式，关于这一块的内容，可以详细参考下文<a href="#%E8%BF%9E%E6%8E%A5%E5%99%A8">连接器</a>一节，？？？</p>
<p>顾名思义，<strong>表模式</strong> 则是定义了在 sql 查询中需要用到的元数据，以及描述了 source 或者 sink 中的数据格式与表定义的映射关系。？？？</p>
<p>下文将详细介绍<strong>连接器</strong>，<strong>数据格式</strong>以及<strong>表模式</strong>，这里我们先看一个例子，将他们串联起来使用：<br>
<strong>DDL:</strong></p>
<pre><code>tableEnvironment.sqlUpdate(
&quot;CREATE TABLE MyTable (\n&quot; +
&quot;  ...    -- declare table schema \n&quot; +
&quot;) WITH (\n&quot; +
&quot;  'connector.type' = '...',  -- declare connector specific properties\n&quot; +
&quot;  ...\n&quot; +
&quot;  'update-mode' = 'append',  -- declare update mode\n&quot; +
&quot;  'format.type' = '...',     -- declare format specific properties\n&quot; +
&quot;  ...\n&quot; +
&quot;)&quot;);
</code></pre>
<p><strong>java/scala:</strong></p>
<pre><code>tableEnvironment
.connect(...)
.withFormat(...)
.withSchema(...)
.inAppendMode()
.createTemporaryTable(&quot;MyTable&quot;)
</code></pre>
<p><strong>YAML:</strong></p>
<pre><code>name: MyTable
type: source
update-mode: append
connector: ...
format: ...
schema: ...
</code></pre>
<p>不同的表类型（source，sink 或者都是）决定了表的不同的注册方式，这里值得一提的是，如果我们使用同一个表名同时注册了 source 和 sink，那意味着我们可以像在操作关系型数据库表一样在同一张表上进行写入和查询操作。<br>
对于流式数据来说，[更新模式](# 更新模式)定义了动态表和外部系统的连接方式，以便进行持续查询。<br>
？？？</p>
<h1 id="表模式">表模式</h1>
<p>表模式定义了表的字段及其类型，类似于 SQL CREATE TABLE 申明中对列的定义。此外，表模式还可以定义如何将外部系统的数据格式与表模式中定义的字段进行映射，这在我们定义的字段名与外部系统数据格式不一致时显得格外重要，例如，kafka 定义了一个 json数据格式，其中有一个字段 username，但是我们表模式中的字段名叫 user，此时便可以使用 from 方法将 username 映射未 user。此外，数据类型也是需要去做映射的，这确保仅将具有有效数据格式的数据写入外部系统。</p>
<p>下面的例子展示了一个简单的，字段一对一映射，且不包含时间属性的表模式定义：<br>
<strong>java/scala：</strong></p>
<pre><code>.withSchema(
	new Schema()
		.field(&quot;MyField1&quot;, DataTypes.INT())     // required: specify the fields of the table (in this order)
		.field(&quot;MyField2&quot;, DataTypes.STRING())
		.field(&quot;MyField3&quot;, DataTypes.BOOLEAN())
)
</code></pre>
<p>对于每个字段，除列的名称和类型外，还可以声明以下属性：<br>
<strong>java/scala：</strong></p>
<pre><code>.withSchema(
 	new Schema()
		.field(&quot;MyField1&quot;, DataTypes.TIMESTAMP(3))
  		.proctime()      // optional: declares this field as a processing-time attribute
		.field(&quot;MyField2&quot;, DataTypes.TIMESTAMP(3))
 		.rowtime(...)    // optional: declares this field as a event-time attribute
		.field(&quot;MyField3&quot;, DataTypes.BOOLEAN())
  		.from(&quot;mf3&quot;)     // optional: original field in the input that is referenced/aliased by this field
)
</code></pre>
<p>使用无界流表时，时间属性至关重要。 因此，处理时间和事件时间（也称为“行时间”）属性都可以定义为表模式的一部分。<br>
有关时间的内容，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/time_attributes.html">事件时间</a>。</p>
<h2 id="rowtime-属性">rowtime 属性</h2>
<h3 id="时间提取器">时间提取器</h3>
<p>rowtime 属性是 table 中对事件时间的实现形式，flink 提供了预定义的时间戳提取器和 watermarker 策略。<br>
有如下时间提取器：<br>
<strong>java/scala：</strong></p>
<pre><code>// 将已存在的时间字段转换为 rowtime
.rowtime(
new Rowtime()
	.timestampsFromField(&quot;ts_field&quot;)    // required: original field name in the input
)

// Converts the assigned timestamps from a DataStream API record into the rowtime attribute
// and thus preserves the assigned timestamps from the source.
// This requires a source that assigns timestamps (e.g., Kafka 0.10+).
.rowtime(
new Rowtime()
		.timestampsFromSource()
)

// 通个继承`org.apache.flink.table.sources.tsextractors.TimestampExtractor`.实现自定义时间提取器
.rowtime(
	new Rowtime()
		.timestampsFromExtractor(...)
)
</code></pre>
<h3 id="watermark-策略">watermark 策略</h3>
<p><strong>java/scala：</strong></p>
<pre><code>// Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum
// observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp
// are not late.
.rowtime(
new Rowtime()
		.watermarksPeriodicAscending()
)

// Sets a built-in watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.
// Emits watermarks which are the maximum observed timestamp minus the specified delay.
.rowtime(
new Rowtime()
	.watermarksPeriodicBounded(2000)    // delay in milliseconds
)

// Sets a built-in watermark strategy which indicates the watermarks should be preserved from the
// underlying DataStream API and thus preserves the assigned watermarks from the source.
.rowtime(
new Rowtime()
	.watermarksFromSource()
)
</code></pre>
<p>我们需要确保在表模式中定义时间属性和 watermarkers。尤其在与基于时间的操作中。</p>
<h2 id="类型字符串">类型字符串</h2>
<p>上面的例子可以看出，基于 java 和 scala 的编程语言使用 DataType 进行字段类型定义，而像 YAML 这样基于配置的实现方式，则是像在 SQL 中的 DDL 一样通过类型字符串进行定义，如：<br>
schema:<br>
- name: MyField1<br>
data-type: TIMESTAMP(3)<br>
proctime: true    # optional: boolean flag whether this field should be a processing-time attribute<br>
- name: MyField2<br>
data-type: TIMESTAMP(3)<br>
rowtime: ...      # optional: wether this field should be a event-time attribute<br>
- name: MyField3<br>
data-type: BOOLEAN<br>
from: mf3</p>
<h1 id="更新模式">更新模式</h1>
<p>对于流式查询来说，我们必须定义动态表与外部连接器之间数据交互的模式，我们称作更新模式，更新模式指定了应与外部系统交换的消息类型，有以下三种更新模式：</p>
<ul>
<li><strong>append 模式</strong>：使用该模式，只有 INSERT 消息类型可以进行交互；</li>
<li><strong>retract 模式</strong>：该模式下，可以交换 ADD 和 RETRACT 消息。 INSERT 更改编码成 ADD 类型消息，而 DELETE 更改则被编码成 RETRACT 类型消息；UPDATE 更改编码为 RETRACT 消息，作用于之前被更新的行，ADD 消息则作用于最新行。与 upsert 模式相反，该模式不需要定义 key，但每一次变更都会被编码成两个消息，因此效率比较低。</li>
<li><strong>upsert 模式</strong>：该模式下，可以交换 UPSERT 和 DELETE 消息。upsert 模式需要定义唯一的 key（可以是组合 key），通过 key 进行更新。为了能够正确应用消息，外部连接器必须知道这个 唯一 key 属性。INSERT 和 UPDATE 变更被当作 UPSERT 消息，DELETE 变更被当作 DELETE消息。和 retract 消息唯一的不同点是。UPDATE 变更被编码为一个消息体，而不是两个。</li>
</ul>
<p><strong>注</strong>：下节连接器一文中列出了每个连接器对应所支持的更新模式。<br>
<strong>例子：</strong><br>
<strong>DDL：</strong></p>
<pre><code>CREATE TABLE MyTable (
	...
) WITH (
	 'update-mode' = 'append'  -- otherwise: 'retract' or 'upsert'
)
</code></pre>
<p><strong>java/scala：</strong></p>
<pre><code>.connect(...)
 .inAppendMode()  
</code></pre>
<p>想要了解更多，可参考文章：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/dynamic_tables.html#continuous-queries">流概念中动态表一文</a></p>
<h1 id="连接器-2">连接器</h1>
<p>flink 提供了很多连接器，但需要注意的是，不是所有的连接器都同时支持流处理和批处理。也不是每一个流式连接器都支持所有的流处理模式，因此，下文为每一个连接器打上了标签，表示其支持的功能特性或者操作类型，或者其他需要我们关注的点。例如，格式标签代表连接器支持的格式类型。</p>
<h2 id="文件系统连接器">文件系统连接器</h2>
<p><mark>Source: Batch</mark> <mark>Source: Streaming Append Mode</mark> <mark>Source: Batch </mark> <mark>Sink: Streaming Append Mode</mark> <mark>Format: OldCsv-only</mark></p>
<h2 id="kafka-连接器">kafka 连接器</h2>
<p><mark>Source: Streaming Append Mode </mark> <mark>Sink: Streaming Append Mode</mark> <mark>Format: CSV, JSON, Avro</mark></p>
<h2 id="elasticsearch-连接器">Elasticsearch 连接器</h2>
<p><mark>Source: Streaming Append Mode </mark> <mark>Sink: Streaming Upsert Mode</mark> <mark>Format: JSON-only</mark></p>
<h2 id="hbase-连接器">HBase 连接器</h2>
<p><mark>Source: Batch </mark> <mark>Sink: Batch </mark> <mark>Sink: Streaming Append Mode</mark> <mark>Sink: Streaming Upsert Mode</mark> <mark>Temporal Join: Sync Mode</mark></p>
<h2 id="jdbc-连接器">JDBC 连接器</h2>
<p><mark>Source: Batch </mark> <mark>Sink: Batch </mark> <mark>Sink: Streaming Append Mode</mark> <mark>Sink: Streaming Upsert Mode</mark> <mark>Temporal Join: Sync Mode</mark></p>
<h2 id="hive-连接器">Hive 连接器</h2>
<p><mark>Source: Batch </mark> <mark>Sink: Batch </mark></p>
<h1 id="表数据格式">表数据格式</h1>
<p>如上连接器中的标签所示，flink 为用户提供了与连接器搭配使用的数据交换格式，下面具体讲解一下：</p>
<h2 id="csv">CSV</h2>
<p>csv 支持序列化和反序列化</p>
<h2 id="json">JSON</h2>
<p>支持序列化和反序列化</p>
<h2 id="apache-avro">Apache Avro</h2>
<p>支持序列化和反序列化</p>
<h2 id="old-csv">Old CSV</h2>
<p>略</p>
<p>#其他连接器<br>
除了以上所讲的连接器外，flink还提供了一些没有迁移（或没有完全迁移完成）到的新的统一接口的 source 和 sink。具体请参考官网原文<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html#further-tablesources-and-tablesinks">https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html#further-tablesources-and-tablesinks</a></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E4%BE%9D%E8%B5%96">依赖</a>
<ul>
<li><a href="#%E8%BF%9E%E6%8E%A5%E5%99%A8">连接器</a></li>
</ul>
</li>
<li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li>
<li><a href="#%E8%A1%A8%E6%A8%A1%E5%BC%8F">表模式</a>
<ul>
<li><a href="#rowtime-%E5%B1%9E%E6%80%A7">rowtime 属性</a>
<ul>
<li><a href="#%E6%97%B6%E9%97%B4%E6%8F%90%E5%8F%96%E5%99%A8">时间提取器</a></li>
<li><a href="#watermark-%E7%AD%96%E7%95%A5">watermark 策略</a></li>
</ul>
</li>
<li><a href="#%E7%B1%BB%E5%9E%8B%E5%AD%97%E7%AC%A6%E4%B8%B2">类型字符串</a></li>
</ul>
</li>
<li><a href="#%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%BC%8F">更新模式</a></li>
<li><a href="#%E8%BF%9E%E6%8E%A5%E5%99%A8-2">连接器</a>
<ul>
<li><a href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%BF%9E%E6%8E%A5%E5%99%A8">文件系统连接器</a></li>
<li><a href="#kafka-%E8%BF%9E%E6%8E%A5%E5%99%A8">kafka 连接器</a></li>
<li><a href="#elasticsearch-%E8%BF%9E%E6%8E%A5%E5%99%A8">Elasticsearch 连接器</a></li>
<li><a href="#hbase-%E8%BF%9E%E6%8E%A5%E5%99%A8">HBase 连接器</a></li>
<li><a href="#jdbc-%E8%BF%9E%E6%8E%A5%E5%99%A8">JDBC 连接器</a></li>
<li><a href="#hive-%E8%BF%9E%E6%8E%A5%E5%99%A8">Hive 连接器</a></li>
</ul>
</li>
<li><a href="#%E8%A1%A8%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F">表数据格式</a>
<ul>
<li><a href="#csv">CSV</a></li>
<li><a href="#json">JSON</a></li>
<li><a href="#apache-avro">Apache Avro</a></li>
<li><a href="#old-csv">Old CSV</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'c1cd96b713d8487407c2',
    clientSecret: 'c0321530de2e8cfb94c7acf916a996cebafb006d',
    repo: 'onepiece-dz.github.io',
    owner: 'onepiece-dz',
    admin: ['onepiece-dz'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://onepiece-dz.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
